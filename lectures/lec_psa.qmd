---
title: "9. Systematic Approaches to Understanding Model Uncertainty"
format:
  revealjs:
    theme: slides.scss
    incremental: false
    slide-number: true
    logo: ../vchem.png
    html-math-method: katex
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    footer: |
      [Back to Website](../index.html)
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
bibliography: references.bib  
---

# Learning Objectives and Outline

## Learning Objectives

::: incremental
1.  Discuss common sources of uncertainty in decision models.
2.  Explain how to draw parameter values from an uncertainty distribution.
3.  Understand inputs and outputs of a PSA
4.  Characterize decision uncertainty using cost-effectiveness acceptability curves and frontiers.
:::

## Lecture Outline

::: incremental
1.  Why Do We Study Uncertainty in a Decision Model?
2.  How Do We Conduct a Probabilistic Sensitivity Analysis?
3.  How Do We Summarize PSA Results?
:::

# 1. Why Do We Study Uncertainty in a Decision Model?

## Two Fundamental Questions of Decision Analysis

::: incremental
1.  Which strategies are cost-effective given constraints and values and *based on current evidence*?
:::

## Two Fundamental Questions of Decision Analysis

1.  Which strategies are cost-effective given constraints and values and *based on current evidence*?

::: incremental
-   Model outcomes (e.g., ICERs) will be sensitive to all sources of uncertainty.
-   **Key Question**: Does this sensitivity *change decisions*?
:::

## Two Fundamental Questions of Decision Analysis

2.  Should we invest more resources to reduce uncertainty in our decisions?

## Role of Probabilistic Sensitivity Analysis in a Decision Model

::: incremental
-   Quantify the degree of decision uncertainty in our model.
-   Is it worth pursuing additional research to reduce uncertainty?
:::

## Different Types of Uncertainty {.smaller}

::: incremental
1.  **First-order**: Stochastic uncertainty from simulating individual patients.

-   Each patient will have a different "experience" in the model, which will create variation in model outputs (e.g., total costs, QALYS) both *within* the model and *across* different model runs.
-   Not relevant for Markov cohort models because those models are **deterministic**---they capture the average experience of a population, and do not simulate individual patient trajectories within that population.
-   Relevant source of uncertainty for discrete event simulation and microsimulation models.
-   Can often be minimized via modeling choices (i.e., simulate a lot of patients!)
:::

## Different Types of Uncertainty {.smaller}

1.  **First-order**: Stochastic uncertainty from simulating individual patients.

::: incremental
2.  **Second-order**: Uncertainty in the "true" value of underlying parameters.

-   Model parameters are often estimated with uncertainty (e.g., 95% confidence interval)
-   You may have assumed or calibrated parameters not rooted in a published research study; there is uncertainty involved in these processes, too!
:::

## Different Types of Uncertainty {.smaller}

1.  **First-order**: Stochastic uncertainty from simulating individual patients.
2.  **Second-order**: Uncertainty in the "true" value of underlying parameters.

::: incremental
3.  **Model structure uncertainty**: Different choices on how to construct the structure of your model will result in different outcome estimates.

-   Different choices for cycle correction (e.g., half-cycle, Simpson's 1/3, etc.)
-   Different choices for how to construct transition probability matrices (e.g., rate-to-probability conversion formulas vs. embedding via Matrix exponentiation)
:::

## Heterogeneity vs. Uncertainty

-   Uncertainty: variation in model outputs due to stochastic experiences of patients, sensitivity to input parameter values, etc.
-   Heterogeneity: variation in model outputs due to differences in patient characteristics.

## Heterogeneity vs. Uncertainty

![](images/paste-E8F213DC.png)

Source: [Briggs et al., "Model Parameter Estimation and Uncertainty: A Report of the ISPOR-SMDM Modeling Good Research Practices Task Force-6"](https://www.ispor.org/docs/default-source/resources/outcomes-research-guidelines-index/model_parameter_estimation_and_uncertainty-6.pdf?sfvrsn=8bc10c8e_0)

## When Does Uncertainty Matter?

```{r}
#| eval: false
#| warning: false
#| message: false 
library(tidyverse)
library(ggthemes)
library(gganimate)
library(patchwork)

df <- tibble(x = -1, y = 1 , scenario = 1, colour = "red")
df_2 <- tibble(x = rnorm(n = 200, mean = -1, sd=0.2), 
              y = rnorm(n=200, mean=1, sd=0.1),
              scenario = 2) %>% 
  mutate(colour = ifelse(x<=0,"red","blue")) %>% 
  mutate(seq = row_number())
df_3 <- tibble(x = pmin(2,pmax(-2,rnorm(n = 200, mean = -.5, sd=1))), 
              y = pmax(-2,pmin(2,rnorm(n=200, mean=1, sd=0.1))),
              scenario = 3) %>% 
  mutate(colour = ifelse(x<=0,"red","blue")) %>% 
  mutate(seq = row_number())


p0 <- 
df %>% 
  ggplot(aes(x = y , y = x)) + geom_point(aes(colour = colour),size=3) +
  theme_tufte() + 
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank()) + 
  geom_errorbar(aes(ymin = x-1, ymax=x+1, x=y-.5, width =0.2)) +
  geom_errorbar(aes(ymin = x+1, ymax=x+3, x=y-.5, width =0.2)) +
  coord_flip() +
  scale_x_continuous(limits = c(-4,4)) +
  scale_y_continuous(limits = c(-4, 4), breaks = c(-1,1)) +
  theme(legend.position = "none")

tmp_0  <- p0 +   annotate("text",x = 0,y=-1,label="Base Case\nDecision",size=5) +
   annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) +
    scale_colour_manual(values = c("red"))
tmp_0 
ggsave(here::here("lectures/media/uncertainty-decision-base.png"),width=10, height=8)


p0 + geom_point(data = df_2 %>% filter(row_number()==1), alpha = 1,aes(colour = colour)) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) + 
    scale_colour_manual(values = c("red"))
ggsave(here::here("lectures/media/uncertainty-decision-base-1.png"),width=10, height=8)

p0 + geom_point(data = df_2 %>% filter(row_number() %in% 1:2), alpha = 1,aes(colour = colour)) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5)+
   scale_colour_manual(values = c("red"))
ggsave(here::here("lectures/media/uncertainty-decision-base-2.png"),width=10, height=8)
 
 p0 + geom_point(data = df_2 %>% filter(row_number() %in% 1:3), alpha = 1,aes(colour = colour)) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) +
    scale_colour_manual(values = c("red"))
ggsave(here::here("lectures/media/uncertainty-decision-base-3.png"),width=10, height=8)

uncert0_anim <- p0 + geom_point(data = df_2 %>% filter(row_number() %in% 1:3), alpha = 1,aes(colour = colour)) +
  geom_point(data = df_2 %>% filter(row_number() %in% 3:100) , alpha = 1,aes(colour = colour)) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
   scale_colour_manual(values = c("red")) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) + 
  transition_states(seq, state_length = 0); 
tmp <- animate(uncert0_anim, duration = 10, fps = length(3:100)/10, renderer = gifski_renderer())
anim_save(here::here("lectures/media/uncertainty-decision-psa-1.gif"), tmp)

tmp_ <- p0 + geom_point(data = df_2, alpha = 0.1,aes(colour = colour)) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) +
   scale_colour_manual(values = c("red"))
tmp_ 
ggsave(here::here("lectures/media/uncertainty-decision-psa-1.png"),width=10, height=8)

# uncert1_anim <- p0 + geom_point(data = df_2 %>% filter(row_number() %in% 1:3), alpha = 1,aes(colour = colour)) +
#   geom_point(data = df_2 %>% filter(row_number() %in% 3:100) , alpha = 1,aes(colour = colour)) +
#   annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
#   annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) + 
#   transition_states(seq, state_length = 0); 
# tmp <- animate(uncert0_anim, duration = 10, fps = length(3:100)/10, renderer = gifski_renderer())
# anim_save(here::here("lectures/media/uncertainty-decision-psa-1.gif"), tmp)


uncert2_anim <- p0 + geom_point(data = df_3 %>% filter(row_number() %in% 1:3), alpha = 1,aes(colour = colour)) +
  geom_point(data = df_3 %>% filter(row_number() %in% 3:100) , alpha = 1,aes(colour = colour)) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) + 
  transition_states(seq, state_length = 0) + 
  scale_colour_manual(values = c("blue","red"))
tmp <- animate(uncert2_anim, duration = 10, fps = length(3:100)/10, renderer = gifski_renderer()) 
anim_save(here::here("lectures/media/uncertainty-decision-psa-2.gif"), tmp)

  
tmp_2 <- p0 +   geom_point(data = df_3, alpha=0.1, aes(colour = colour)) +
  scale_colour_manual(values = c("blue","red")) +
  annotate("text",x = 0,y=-1,label="Decision is Same\nAs Base Case",size=5) +
  annotate("text",x = 0,y=1,label="Alternative\nDecision",size=5) 
tmp_2 
ggsave(here::here("lectures/media/uncertainty-decision-psa-2.png"),width=10, height=8)



```

## When Does Uncertainty Matter?

![](media/uncertainty-decision-base.png){height="800px" fig-align="center"}

## When Does Uncertainty Matter?

![](media/uncertainty-decision-base-1.png){height="800px" fig-align="center"}

## When Does Uncertainty Matter?

![](media/uncertainty-decision-base-2.png){height="800px" fig-align="center"}

## When Does Uncertainty Matter?

![](media/uncertainty-decision-base-3.png){height="800px" fig-align="center"}

## When Does Uncertainty Matter?

In this example, model outputs are sensitive to uncertainty, but **decisions are not**.

![](media/uncertainty-decision-psa-1.gif){height="800px" fig-align="center"}

## When Does Uncertainty Matter?

In this example, model outputs are sensitive to uncertainty, but **decisions are not**.

![](media/uncertainty-decision-psa-1.png){height="600px" fig-align="center"}

## When Does Uncertainty Matter?

Both model **outputs** *and* **decisions** are sensitive to uncertainty.

![](media/uncertainty-decision-psa-2.gif){height="800px" fig-align="center"}

## When Does Uncertainty Matter?

Both model **outputs** *and* **decisions** are sensitive to uncertainty.

![](media/uncertainty-decision-psa-2.png){height="600px" fig-align="center"}

## Markov Cohort Models

![](media/uncertainty-decision-markov.png){height="500px" fig-align="center"}

::: footer
Note that this picture represents common scenarios; uncertainty may be greater or lesser in any particular modeling context.
:::

## DES and Microsimulation

![](media/uncertainty-decision-des.png){height="500px" fig-align="center"}

::: footer
Note that this picture represents common scenarios; uncertainty may be greater or lesser in any particular modeling context. DES = discrete event simulation
:::

```{r}
source(here::here("_healthy-sick-dead/01-healthy-sick-dead_setup.r"))
source(here('_healthy-sick-dead/functions_healthy-sick-dead.r'))
```

```{r, cache=TRUE}
run_psa = TRUE
options(knitr.kable.NA = '')


source(here("_healthy-sick-dead/02_healthy-sick-dead_process-parameters.r"))
if (run_psa) source(here("_healthy-sick-dead/03_healthy-sick-dead_process-parameters_psa.r"))
source(here("_healthy-sick-dead/04_healthy-sick-dead_discounting-and-cycle-adjustments.r"))
source(here("_healthy-sick-dead/05_healthy-sick-dead_construct-transition-probability-matrices.r"))
if (run_psa) source(here("_healthy-sick-dead/05b_healthy-sick-dead_construct-transition-probability-matrices-psa.r"))
source(here("_healthy-sick-dead/06_healthy-sick-dead_process-payoffs.r"))
if (run_psa) source(here("_healthy-sick-dead/06b_healthy-sick-dead_process-payoffs-psa.r"))
source(here("_healthy-sick-dead/07_healthy-sick-dead_calculate-markov-trace.r"))
if (run_psa) source(here("_healthy-sick-dead/07b_healthy-sick-dead_calculate-markov-trace-psa.r"))
source(here("_healthy-sick-dead/08_healthy-sick-dead_cea-results-base-case.r"))
if (run_psa) source(here("_healthy-sick-dead/08b_healthy-sick-dead_cea-results-psa.r"))

```

## Probabilistic Sensitivity Analysis

![](media/uncertainty-decision-psa.png){height="150px" fig-align="center"}

# 2. How Do We Conduct Probabilistic Sensitivity Analyses? {background-image="media/psa-distribution-plot.png" data-background-size="contain" background-opacity="0.1"}

## Idea {background-image="media/psa-distribution-plot.png" data-background-size="contain" background-opacity="0.1"}

::: incremental
-   Run the model many times, each time drawing a given parameter value from its uncertainty distribution.
-   Collect the parameter values and model outputs (e.g., total costs and QALYs) in a **probabilistic sensitivity analysis (PSA)** dataset.
-   Analyze the PSA results to construct uncertainty estimates for ICERs, NMB/NHB, etc.
-   PSA results can also be used for **value of information** that quantify decision uncertainty and the value of future research to reduce uncertainty.
:::

##  {background-image="media/psa-distribution-plot.png" data-background-size="contain"}

```{r ucertfig, eval = FALSE}

  # params_psa %>% 
  #   bind_cols() %>% 
  #   mutate(i = row_number()) %>% 
  #   gather(parameter, value,-i) %>% 
  #   group_by(parameter)  %>% 
  #   summarise(mean_psa=mean(value)) %>% 
  #   left_join(params %>% bind_cols() %>% gather(parameter,base_case),"parameter") %>% 
  #   mutate(diff = abs((mean_psa - base_case)/base_case)) %>% 
  #   arrange(desc(diff)) %>% 
  #   knitr::kable() %>% kableExtra::kable_styling()

  params_psa %>% 
    bind_cols() %>% 
    mutate(i = row_number()) %>% 
    gather(parameter, value,-i) %>% 
    group_by(parameter) %>% 
    nest() %>% 
    mutate(density = map(data,~(
      {.x %>% ggplot(aes(x=value)) + geom_density()} %>% ggplot_build() %>% pluck("data") %>% pluck(1) %>% 
        select(x,y) %>% as_tibble() %>% mutate(nrow = nrow(.))
    ))) %>% 
    select(-data) %>% 
    unnest(cols = density) %>% 
    
    ungroup() %>% 
    filter(!grepl("^n_|^d_",parameter)) %>% 
    left_join(params %>% bind_cols() %>% gather(parameter,base_case),"parameter") %>%  
    left_join({params_psa %>% 
        bind_cols() %>% 
        mutate(i = row_number()) %>% 
        gather(parameter, value,-i) %>% 
        group_by(parameter)  %>% 
        summarise(mean_psa=mean(value))},"parameter") %>% 
    ggplot(aes(x = x, y = y)) + geom_line() + 
    facet_wrap(~parameter,scales="free")  +
    geom_vline(aes(xintercept=base_case), colour = "black",lwd=2) +
    ggthemes::theme_clean() + 
    scale_x_continuous(guide = guide_axis(n.dodge=3)) + 
    labs(x = "", y = "")
  
  ggsave(here::here("lectures/media/psa-distribution-plot.png"),width=10,height=8)
  
```

```{r animpdf, eval=FALSE}

library(tidyverse)
library(data.table)
library(gganimate)
set.seed(12)
df_norm <- tibble(x=rnorm(10000,mean = 10, sd=3))
df_lnorm <- tibble(x = rlnorm(10000,mean=log(3),sd=0.01)); plot(density(df_lnorm$x))
df_gamma <- tibble(x = rgamma(10000,shape = 44.4, scale=22.5)); plot(density(df_gamma$x))
df_unif <- tibble(x = runif(1e6,min = 0.3,max=0.4)); plot(density(df_unif$x))

animate_pdf_to_cdf <- function(df, scene_to_show = NULL, q_ = c(0.01,  0.25, 0.5, 0.75,  0.99), breaks_to_show = NULL ) {
  pdf <-
    {df %>% ggplot(aes(x = x)) + geom_density()} %>%
    ggplot_build() %>% pluck("data") %>% pluck(1) %>%
    as_tibble() %>%
    select(x,y) %>%
    data.table(key = "x")
  cdf <-
    {df %>% ggplot(aes(x = x)) + stat_ecdf()} %>%
    ggplot_build() %>% pluck("data") %>% pluck(1) %>%
    as_tibble() %>%
    select(x,y) %>%
    data.table(key = "y")

  
  q <- q_ %>%
    map_dbl(~{
      quantile(df$x,.x)
    }) %>%
    set_names(q_) %>%
    data.frame() %>%
    set_names("x_") %>%
    rownames_to_column(var = "quantile") %>%
    as.data.table(key = "x_")

  scene2 <-
    pdf[q,roll="nearest"] %>%
    as_tibble() %>%
    select(y,quantile) %>%
    right_join(pdf,c("y")) %>%
    arrange(x,y) %>%
    mutate(scene = 2) %>%
    mutate(quantile = glue::glue("{as.numeric(quantile)}"))

  q2 <-
    q %>%
    as_tibble() %>%
    mutate(quantile = as.numeric(quantile)) %>%
    data.table(key="quantile")

  scene3 <-
    cdf[q2,roll="nearest"] %>%
    as_tibble() %>%
    set_names(c("x","quantile","x_")) %>%
    select(-x_) %>%
    right_join(cdf,"x") %>%
    arrange(x,y)  %>%
    mutate(scene = 3) %>%
    mutate(quantile = glue::glue("{as.numeric(quantile)}")) %>%
    ungroup() %>%
    filter(!is.na(as.numeric(quantile)) | row_number() %in% sample(1:nrow(.),1000)); dim(scene2)

  p_df <-
    scene2 %>%
    mutate(scene = 1) %>%
    bind_rows(scene2) %>%
    bind_rows(scene3) %>%
    bind_rows(scene3 %>% mutate(scene=4)) %>%
    #bind_rows(scene3 %>% mutate(scene=5)) %>%
    #bind_rows(scene3 %>% mutate(scene=6)) %>%
    mutate(quantile = ifelse(quantile=="NA",NA, quantile))

  p_df_label =
    p_df %>% filter(!is.na(quantile)) %>%
    mutate(y = ifelse(scene==1,0,y)) %>%
    mutate(x = ifelse(scene==4,min(scene3$x),x)) %>%
    mutate(y = ifelse(scene==6,0,y)) %>%
    mutate(quantile = glue::glue(" q{quantile}"))

  if (!is.null(scene_to_show)) {
    p_df <- 
      p_df %>% filter(scene==scene_to_show)
    p_df_label <- 
      p_df_label %>% filter(scene==scene_to_show)
    p <-
      p_df %>% ggplot(aes(x = x, y = y))+ geom_line() + geom_text(data = p_df_label , aes(label = quantile, group= quantile),size=3)+
      scale_y_continuous(breaks = breaks_to_show) +
      scale_x_continuous(breaks = q$x_, labels = round(q$x_,2),guide = guide_axis(n.dodge=3)) +
      ggthemes::theme_clean(base_size=15) +
      labs(x = "Parameter Value", y = "") 
  } else {
      p <-
        p_df %>% ggplot(aes(x = x, y = y))+ geom_line() + geom_text(data = p_df_label , aes(label = quantile, group= quantile))+
        scale_y_continuous(breaks = seq(0,1,0.1)) +
        scale_x_continuous(breaks = q$x_, labels = round(q$x_,2)) +
        ggthemes::theme_clean() +
        labs(x = "Parameter Value", y = "") + #facet_wrap(~scene, scales="free"); p
        gganimate::transition_states(scene) + gganimate::view_follow()
  }

  return(p)

}

df_norm %>% animate_pdf_to_cdf(scene = 2)
ggsave(here::here("lectures/media/pdf-to-cdf-norm.png"),height=4, width=5)
df_norm %>% animate_pdf_to_cdf(scene = 3,seq(0,1,0.1)) 
ggsave(here::here("lectures/media/pdf-to-cdf-norm4.png"),height=4, width=5)
set.seed(23)
df_norm %>% animate_pdf_to_cdf(scene = 3, q = round(runif(5, min = 0, max = 1),3))
ggsave(here::here("lectures/media/pdf-to-cdf-norm_random1.png"),height=4, width=5)
anim_norm <- df_norm %>% animate_pdf_to_cdf(breaks_to_show = seq(0,1,0.1))
anim_save(here::here("lectures/media/pdf-to-cdf-norm.gif"), anim_norm)

anim_gamma1 <- df_gamma %>% animate_pdf_to_cdf(scene = 2)
ggsave(here::here("lectures/media/pdf-to-cdf-gamma.png"),height=5, width=5)
anim_gamma3 <- df_gamma %>% animate_pdf_to_cdf(scene = 4)
ggsave(here::here("lectures/media/pdf-to-cdf-gamma4.png"),height=5, width=5)
anim_gamma <- df_gamma %>% animate_pdf_to_cdf()
anim_save(here::here("lectures/media/pdf-to-cdf-gamma.gif"), anim_gamma)

anim_lnorm1 <- df_lnorm %>% animate_pdf_to_cdf(scene = 2)
ggsave(here::here("lectures/media/pdf-to-cdf-lnorm.png"),height=5, width=5)
anim_lnorm3 <- df_lnorm %>% animate_pdf_to_cdf(scene = 4)
ggsave(here::here("lectures/media/pdf-to-cdf-lnorm4.png"),height=5, width=5)
anim_lnorm <- df_lnorm %>% animate_pdf_to_cdf()
anim_save(here::here("lectures/media/pdf-to-cdf-lnorm.gif"), anim_lnorm)

set.seed(123)
r_HS = 	tibble(x = rgamma(1e4,shape = 30,rate = 200),param="r_HS")
r_HD	= tibble(x = rgamma(1e4,shape = 60, rate = 10000),param = "r_HD")
hr_S = 	tibble(x = rlnorm(1e4,log(3), sdlog = 0.01),param = "hr_S")
hr_HS_trtB	= tibble(x=rlnorm(1e4,log(0.96), sdlog = 0.02),param = "hr_HS_trtB")
hr_HS_trtC	= tibble(x=rlnorm(1e4,log(0.9), sdlog = 0.02),param = "hr_HS_trtC")
hr_HS_trtD = 	tibble(x=rlnorm(1e4,log(0.92), sdlog = 0.02),param = "hr_HS_trtD")
hr_HS_trtE = 	tibble(x=rlnorm(1e4,log(0.92), sdlog = 0.02), param = "hr_HS_trtE")
u_H	= tibble(x=rbeta(1e4,shape1 = 200, shape2 = 3), param = "u_H")
u_S	= tibble(x=rbeta(1e4,shape1 = 130, shape2 = 45), param = "u_S")
c_S	 = tibble(x=rgamma(1e4,shape = 44.4, scale = 22.5), param = "c_S")
c_trtA	=tibble(x= rgamma(1e4,shape = 12.5, scale = 2), param = "c_trtA")
c_trtB	=tibble(x= rgamma(1e4,shape = 12, scale = 83.3), param = "c_trtB")
c_trtC	=tibble(x= rgamma(1e4,shape = 36.144, scale = 83), param = "c_trtC")
c_trtD	=tibble(x= rgamma(1e4,shape = 14.458, scale = 83), param = "c_trtD")
c_trtE	=tibble(x= rgamma(1e4,shape = 60.24, scale = 83), param = "c_trtE")


params_lut <- c("r_HS = gamma(shape = 30,rate = 200)", "r_HD = gamma(shape = 60, rate = 10000)", "hr_S = lognormal(meanlog = log(3), sdlog = 0.01)", "hr_HS_trtB = lognormal(meanlog = log(0.96), sdlog = 0.02)", "hr_HS_trtC = lognormal(meanlog = log(0.88), sdlog = 0.02)", "hr_HS_trtD = lognormal(meanlog = log(0.92), sdlog = 0.02)", "hr_HS_trtE = lognormal(meanlog = log(0.92), sdlog = 0.02)",
               "u_H = beta(shape1 = 200, shape2 = 3)","u_S = beta(shape1 = 130, shape2 = 45)","c_S = gamma(shape = 44.4, scale = 22.5)", "c_trtA = gamma(shape = 12.5, scale = 2)","c_trtB = gamma(shape = 12, scale = 83.3)","c_trtC = gamma(shape = 36.144, scale = 83)","c_trtD = gamma(shape = 18.67, scale = 83)","c_trtE = gamma(shape = 60.24, scale = 83)")


params <- list(r_HS, r_HD, hr_S, hr_HS_trtB, hr_HS_trtC, hr_HS_trtD, hr_HS_trtE,
               u_H,u_S,c_S, c_trtA,c_trtB,c_trtC,c_trtD,c_trtE)

pdf_params <- 
  params %>% 
  map2(.,params_lut,~({
    .x %>% animate_pdf_to_cdf(scene = 1) + ggtitle(.y)
    ggsave(here::here(glue::glue("lectures/media/{.x$param[1]}.png")),height=4, width=5)
  })) %>% 
  set_names(c("r_HS:gamma(shape = 30,rate = 200)", "r_HD=gamma(shape = 60, rate = 10000)", "hr_S=lognormal(meanlog = log(3), sdlog = 0.01)", "hr_HS_trtB=lognormal(meanlog = log(0.96), sdlog = 0.02)", "hr_HS_trtC=lognormal(meanlog = log(0.88), sdlog = 0.02)", "hr_HS_trtD=lognormal(meanlog = log(0.92), sdlog = 0.02)", "hr_HS_trtE=lognormal(meanlog = log(0.92), sdlog = 0.02)",
               "u_H=beta(shape1 = 200, shape2 = 3)","u_S=beta(shape1 = 130, shape2 = 45)","c_S=gamma(shape = 44.4, scale = 22.5)", "c_trtA=gamma(shape = 12.5, scale = 2)","c_trtB=gamma(shape = 12, scale = 83.3)","c_trtC=gamma(shape = 36.144, scale = 83)","c_trtD=gamma(shape = 18.67, scale = 83)","c_trtE=gamma(shape = 60.24, scale = 83)"))

cdf_params <- 
  params %>% 
  map2(.,params_lut,~({
    set.seed(23)
    .x %>% animate_pdf_to_cdf(scene = 3,q_ = round(runif(5, min = 0, max = 1),3),breaks_to_show = seq(0,1,0.1)) + ggtitle(.y)
    ggsave(here::here(glue::glue("lectures/media/cdf{.x$param[1]}.png")),height=4, width=5)
  })) %>% 
  set_names(c("r_HS", "r_HD", "hr_S", "hr_HS_trtB", "hr_HS_trtC", "hr_HS_trtD", "hr_HS_trtE",
               "u_H","u_S","c_S", "c_trtA","c_trtB","c_trtC","c_trtD","c_trtE"))


```

## How Do We Draw PSA Values?

::: incremental
-   Central limit tells us that distribution for many estimated parameters is normal.
-   However, often we do not rely on a single parameter estimate, but rather on a range of estimates from the literature.
-   In any PSA, we want to specify parameter uncertainty in such a way as to capture the overall level of uncertainty in model parameters.
:::

## How Do We Draw PSA Values?

| Parameter Type                  | Distribution     |
|---------------------------------|------------------|
| Probability                     | beta             |
| Rate                            | gamma            |
| Utility weight                  | beta             |
| Right skew (e.g., cost)         | gamma, lognormal |
| Relative risks or hazard ratios | lognormal        |
| Odds Ratio                      | logistic         |

## How Do I Draw Values


https://yuhanxuan.shinyapps.io/shiny4dist/



<!-- ## How Do We Draw PSA Values? -->

<!-- ::: r-stack -->
<!-- ![](media/r_HS.png){.fragment height="500px"} -->

<!-- ![](media/r_HD.png){.fragment height="500px"} -->

<!-- ![](media/hr_S.png){.fragment height="500px"} -->

<!-- ![](media/hr_HS_trtB.png){.fragment height="500px"} -->

<!-- ![](media/hr_HS_trtC.png){.fragment height="500px"} -->

<!-- ![](media/hr_HS_trtD.png){.fragment height="500px"} -->

<!-- ![](media/hr_HS_trtE.png){.fragment height="500px"} -->

<!-- ![](media/u_H.png){.fragment height="500px"} -->

<!-- ![](media/u_S.png){.fragment height="500px"} -->

<!-- ![](media/c_S.png){.fragment height="500px"} -->

<!-- ![](media/c_trtA.png){.fragment height="500px"} -->

<!-- ![](media/c_trtB.png){.fragment height="500px"} -->

<!-- ![](media/c_trtC.png){.fragment height="500px"} -->

<!-- ![](media/c_trtD.png){.fragment height="500px"} -->

<!-- ![](media/c_trtE.png){.fragment height="500px"} -->
<!-- ::: -->

<!-- ## How Do We Draw PSA Values -->

<!-- ::: incremental -->
<!-- -   All the model parameters have different uncertainty distributions. How do we draw values? -->
<!-- -   **Inverse Transform Method** -->
<!-- -   Idea: randomly sample from quantiles in the distribution. -->
<!-- ::: -->

<!-- ## How Do We Draw PSA Values -->

<!-- ::: incremental -->
<!-- -   Requires drawing a random number between 0 and 1 (the quantile) -->
<!-- -   Requires an *inverse cumulative density function* -->
<!-- -   You can do this in Excel! -->
<!-- ::: -->

<!-- ## Normal Distribution: PDF -->

<!-- ![](media/pdf-to-cdf-norm.png){fig-align="center" layout-valign="center" height="500px"} -->

<!-- ## Normal Distribution: CDF -->

<!-- ![](media/pdf-to-cdf-norm4.png){fig-align="center" layout-valign="center" height="500px"} -->

<!-- ## Normal Distribution: PDF to CDF -->

<!-- ![](media/pdf-to-cdf-norm.gif){fig-align="center" layout-valign="center" height="500px"} -->

<!-- ```{r} -->
<!-- #| eval: false -->
<!-- anim_norm -->
<!-- ``` -->

<!-- ## Normal Distribution: CDF -->

<!-- ![](media/pdf-to-cdf-norm4.png){fig-align="center" layout-valign="center" height="500px"} -->

<!-- ## Drawing from an Arbitrary PSA Distribution -->

<!-- 1.  Draw a random number between 0 and 1 (`RAND()`) -->
<!-- 2.  Feed this number through the quantile function ("inverse cumulative density function") for the specified distribution. -->
<!-- 3.  Repeat as many times as needed for the PSA (e.g., 1,000 times) -->

## Constructing a PSA Sample

For a given iteration $j$

1.  Draw separate PSA values from the uncertainty distributions in your model.
2.  Run the model and calculate model outputs (e.g., total costs and QALYs for each strategy).
3.  Record the PSA parameter values and the outcome results in a table.
4.  Repeat 1-3 many times.

## Common PSA Distributions in Amua

| Parameter Type                  | Distribution     | Amua                      |
|---------------------------------|------------------|---------------------------|
| Probability                     | beta             | Beta(shape1,shape2,\~)    |
| Rate                            | gamma            | Gamma(shape, scale, \~)   |
| Utility weight                  | beta             | Beta(shape1,shape2,\~)    |
| Right skew (e.g., cost)         | gamma, lognormal | LogNorm(shape,scale,\~)   |
| Relative risks or hazard ratios | lognormal        | LogNorm(shape,scale,\~)   |
| Odds Ratio                      | logistic         | Logistic(location, scale) |

## Exmample: Uncertainty in Utility Weight

- Base case value: 0.95
- Sample from `Beta(95,5,~)`
- Alternatively, sample from `Beta(950,50,~)`

## Exmample: Uncertainty in Utility Weight

```{r, echo = FALSE}
#| fig-width: 8
#| fig-height: 5
#| fig-align: center
p = tibble(samp1 = rbeta(95,5,n=1e4), samp2 = rbeta(950,50,n=1e4)) %>% 
  mutate(index = row_number()) %>% 
  gather(samp,value,-index) %>% 
  mutate(samp = case_when(samp == "samp1" ~ "Beta(95,5,~)", 
                          samp == "samp2" ~ "Beta(950,50,~")) %>% 
  ggplot() + geom_density(aes(group = samp, x = value, colour =samp)) + 
  hrbrthemes::theme_ipsum() + ggsci::scale_colour_d3() + 
  labs(title = "Utility Weight Uncertainty", x = "Utility Weight Value", y = "Density") + 
  geom_vline(aes(xintercept = 0.95))

p_labeled <- directlabels::direct.label(p, "top.points") + scale_y_continuous(limits = c(0,70))
print(p_labeled)
```

# Interactive Amua Session 

# 3. How Do We Summarize PSA Results?

## How Do We Summarize PSA Results?

::: columns
::: column
-   Plot costs and QALYs of each iteration to show degree of variation in estimates.
-   Figure plots values at each iteration, the average across 1,000 iterations (large points) and ellipses that capture \~95% of points.
:::

::: column
```{r}
#| fig.height=10
plot.psa(l_psa) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  xlab("Effectiveness (QALYs)") +
  guides(col = guide_legend(nrow = 2)) +
  theme(legend.position = "bottom")
```
:::
:::


## Cost Effectiveness Acceptability Curves

-   CEACs summarize the degree of uncertainty as captured by our PSA.

-   CEAC represents the (Bayesian) probability of each option being cost-effective at different levels of the cost-effectiveness threshold $\lambda$.

[Source](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1524-4733.2008.00358.x)

## Cost Effectiveness Acceptability Curves

```{r}
plot.ceac(ceac_obj, frontier=FALSE) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme(legend.position = c(0.82, 0.5))

```

## Constructing the CEAC

::: columns
::: column
1.  Define a WTP value.
:::

::: column
$\lambda = 50000$
:::
:::

## Constructing the CEAC

2.  Use the PSA sample to calculate the **Net Monetary Benefit (NMB)** and/or the **Net Health Benefit (NHB)** of each strategy.

Net Monetary Benefit

$$
TOTQALY * \lambda - TOTCOST
$$

$$
Net Health Benefit $$ TOTQALY - \frac{TOTCOST}{\lambda} \$\$

## PSA Sample

::: {style="font-size: 0.8em"}
```{r}
library(tidyverse)
library(knitr)
library(kableExtra)
df_psa <- 
tibble::tribble(
  ~totcost_trtA, ~totcost_trtB, ~totcost_trtC, ~totcost_trtD, ~totcost_trtE, ~totqaly_trtA, ~totqaly_trtB, ~totqaly_trtC, ~totqaly_trtD, ~totqaly_trtE,
    19619.47188,   25588.07151,   37065.37786,   23997.76554,    40797.1777,   18.53716587,   18.61511665,   18.72621501,   18.65291992,   18.65291992,
    11776.92856,   17379.41979,   36873.01518,   19453.58814,   45567.64775,   17.01051936,    17.1082885,   17.19659669,    17.1353478,    17.1353478,
    13292.49032,   19268.95151,   33789.88511,    19886.4207,   41933.20163,   17.25143381,   17.35911175,   17.51076684,   17.46195864,   17.46195864,
    14652.37199,   19102.09981,   25153.62823,   19245.88282,    33977.4242,   16.12758803,   16.13136416,   16.37773559,   16.25371623,   16.25371623,
    13286.67336,   15912.61046,   26997.83009,   18687.76003,   39152.51988,   15.93832559,   16.03653696,   16.14110603,   16.07599929,   16.07599929,
    14958.84595,   17505.56022,   32929.45939,   20602.67016,   49327.29579,   16.01576183,   16.08855226,   16.28681468,   16.18276785,   16.18276785
  ) %>% 
  mutate(PSA_ID = row_number()) %>% 
  select(PSA_ID, everything()) 

df_psa %>% 
  mutate_all(~round(.,4)) %>% 
  kable() %>% 
  kable_styling()
```
:::

## Net Monetary Benefit

::: {style="font-size: 0.8em"}
```{r}
wtp = 50000
df_psa %>% 
  mutate(NMB_A = totqaly_trtA * wtp - totcost_trtA,
         NMB_B = totqaly_trtB * wtp - totcost_trtB,
         NMB_C = totqaly_trtC * wtp - totcost_trtC,
         NMB_D = totqaly_trtD * wtp - totcost_trtD,
         NMB_E = totqaly_trtE * wtp - totcost_trtE) %>% 
  select(starts_with("NMB_")) %>% 
  mutate_all(~round(.,0)) %>% 
  mutate(max = pmax(NMB_A,NMB_B,NMB_C,NMB_D,NMB_E)) %>% 
  mutate(PSA_ID = row_number()) %>% 
  select(PSA_ID, everything())   %>% 
  select(-max) %>% 
  kable(digits=0,'html', booktabs =TRUE, escape = FALSE,caption="Note: Values shown are for a single value of lambda (50,000/QALY)") %>% 
  kable_styling()
```
:::

## Identify the Optimal Strategy

3.  For each iteration, determine which strategy **maximizes** NMB/NHB.

-   This is the optimal strategy for a given $\lambda$ value.

## Identify the Optimal Strategy

::: {style="font-size: 0.8em"}
```{r}
wtp = 50000
df_psa %>% 
  mutate(NMB_A = totqaly_trtA * wtp - totcost_trtA,
         NMB_B = totqaly_trtB * wtp - totcost_trtB,
         NMB_C = totqaly_trtC * wtp - totcost_trtC,
         NMB_D = totqaly_trtD * wtp - totcost_trtD,
         NMB_E = totqaly_trtE * wtp - totcost_trtE) %>% 
  select(starts_with("NMB_")) %>% 
  mutate_all(~round(.,0)) %>% 
  mutate(max = pmax(NMB_A,NMB_B,NMB_C,NMB_D,NMB_E)) %>% 
  mutate(PSA_ID = row_number()) %>% 
  select(PSA_ID, everything()) %>% 

  mutate(
    NMB_A = cell_spec(NMB_A,"html",
                      color = ifelse(NMB_A==max,"white","black"),
                      background = ifelse(NMB_A==max,"red","white"),
                      bold = ifelse(NMB_A==max,T,F))) %>% 
  mutate(
    NMB_B = cell_spec(NMB_B,"html",
                      color = ifelse(NMB_B==max,"white","black"),
                      background = ifelse(NMB_B==max,"red","white"),
                      bold = ifelse(NMB_B==max,T,F))) %>% 
  mutate(
    NMB_C = cell_spec(NMB_C,"html",
                      color = ifelse(NMB_C==max,"white","black"),
                      background = ifelse(NMB_C==max,"red","white"),
                      bold = ifelse(NMB_C==max,T,F))) %>% 
  mutate(
    NMB_D = cell_spec(NMB_D,"html",
                      color = ifelse(NMB_D==max,"white","black"),
                      background = ifelse(NMB_D==max,"red","white"),
                      bold = ifelse(NMB_D==max,T,F))) %>% 
  mutate(
    NMB_E = cell_spec(NMB_E,"html",
                      color = ifelse(NMB_E==max,"white","black"),
                      background = ifelse(NMB_E==max,"red","white"),
                      bold = ifelse(NMB_E==max,T,F))) %>%   
  select(-max) %>% 
  kable(digits=0,'html', booktabs =TRUE, escape = FALSE) %>% 
  kable_styling()
```
:::

## Identify the Optimal Strategy

::: {style="font-size: 0.8em"}
```{r}
ex_ceac_ <- 
  df_psa %>% 
  mutate(NMB_A = totqaly_trtA * wtp - totcost_trtA,
         NMB_B = totqaly_trtB * wtp - totcost_trtB,
         NMB_C = totqaly_trtC * wtp - totcost_trtC,
         NMB_D = totqaly_trtD * wtp - totcost_trtD,
         NMB_E = totqaly_trtE * wtp - totcost_trtE) %>% 
  select(starts_with("NMB_")) %>% 
  mutate_all(~round(.,0)) %>% 
  mutate(max = pmax(NMB_A,NMB_B,NMB_C,NMB_D,NMB_E)) %>% 
  mutate(PSA_ID = row_number()) %>% 
  select(PSA_ID, everything()) %>% 
  mutate(NMB_A = ifelse(NMB_A==max,1,0)) %>% 
  mutate(NMB_B = ifelse(NMB_B==max,1,0)) %>% 
  mutate(NMB_C = ifelse(NMB_C==max,1,0)) %>% 
  mutate(NMB_D = ifelse(NMB_D==max,1,0)) %>% 
  mutate(NMB_E = ifelse(NMB_E==max,1,0)) %>% 
  select(-max) 

ex_ceac <- 
  ex_ceac_ %>% 
  mutate(
    NMB_A = cell_spec(NMB_A,"html",
                      color = ifelse(NMB_A==1,"white","black"),
                      background = ifelse(NMB_A==1,"red","white"),
                      bold = ifelse(NMB_A==1,T,F))) %>% 
  mutate(
    NMB_B = cell_spec(NMB_B,"html",
                      color = ifelse(NMB_B==1,"white","black"),
                      background = ifelse(NMB_B==1,"red","white"),
                      bold = ifelse(NMB_B==1,T,F))) %>% 
  mutate(
    NMB_C = cell_spec(NMB_C,"html",
                      color = ifelse(NMB_C==1,"white","black"),
                      background = ifelse(NMB_C==1,"red","white"),
                      bold = ifelse(NMB_C==1,T,F))) %>% 
  mutate(
    NMB_D = cell_spec(NMB_D,"html",
                      color = ifelse(NMB_D==1,"white","black"),
                      background = ifelse(NMB_D==1,"red","white"),
                      bold = ifelse(NMB_D==1,T,F))) %>% 
  mutate(
    NMB_E = cell_spec(NMB_E,"html",
                      color = ifelse(NMB_E==1,"white","black"),
                      background = ifelse(NMB_E==1,"red","white"),
                      bold = ifelse(NMB_E==1,T,F))) %>%   
  rename(MAX_IS_A = NMB_A,
         MAX_IS_B = NMB_B,
         MAX_IS_C = NMB_C,
         MAX_IS_D = NMB_D,
         MAX_IS_E = NMB_E) 
ex_ceac %>% 
  kable(digits=0,'html', booktabs =TRUE, escape = FALSE) %>% 
  kable_styling()
```
:::

## How Often is the Stratgy the Optimal?

-   The average of this binary indicator across all PSA model runs is the fraction of the time each strategy is optimal *for a given value of* $\lambda$.

## How Often is the Strategy the Optimal?

::: {style="font-size: 0.8em"}
```{r}
ex_ceac_ %>% 
   rename(MAX_IS_A = NMB_A,
         MAX_IS_B = NMB_B,
         MAX_IS_C = NMB_C,
         MAX_IS_D = NMB_D,
         MAX_IS_E = NMB_E)  %>% 
  summarise_at(vars(starts_with("MAX")),mean) %>% 
  mutate(lambda = wtp) %>% 
  select(lambda, everything()) %>% 
  kable(digits=4,'html', booktabs =TRUE, escape = FALSE) %>% 
  kable_styling()
```
:::

## How Often is the Strategy the Optimal?

-   Now repeat this exercise across a range of values for $\lambda$.

## How Often is the Strategy the Optimal?

::: {style="font-size: 0.6em"}
```{r}

ex_ceac_full <- 
  sort(c(50000,seq(20000,200000,20000))) %>% map(~({
    ex_ceac_ <- 
    df_psa %>% 
    mutate(NMB_A = totqaly_trtA * .x - totcost_trtA,
           NMB_B = totqaly_trtB * .x - totcost_trtB,
           NMB_C = totqaly_trtC * .x - totcost_trtC,
           NMB_D = totqaly_trtD * .x - totcost_trtD,
           NMB_E = totqaly_trtE * .x - totcost_trtE) %>% 
    select(starts_with("NMB_")) %>% 
    mutate_all(~round(.,0)) %>% 
    mutate(max = pmax(NMB_A,NMB_B,NMB_C,NMB_D,NMB_E)) %>% 
    mutate(PSA_ID = row_number()) %>% 
    select(PSA_ID, everything()) %>% 
    mutate(NMB_A = ifelse(NMB_A==max,1,0)) %>% 
    mutate(NMB_B = ifelse(NMB_B==max,1,0)) %>% 
    mutate(NMB_C = ifelse(NMB_C==max,1,0)) %>% 
    mutate(NMB_D = ifelse(NMB_D==max,1,0)) %>% 
    mutate(NMB_E = ifelse(NMB_E==max,1,0)) %>% 
    select(-max) 
    
    ex_ceac_ %>% 
     rename(MAX_IS_A = NMB_A,
           MAX_IS_B = NMB_B,
           MAX_IS_C = NMB_C,
           MAX_IS_D = NMB_D,
           MAX_IS_E = NMB_E)  %>% 
    summarise_at(vars(starts_with("MAX")),mean) %>% 
    mutate(lambda = .x) %>% 
    select(lambda, everything())
    
  })) %>% 
    bind_rows()

ex_ceac_full %>% 
  kable(digits=4,'html', booktabs =TRUE, escape = FALSE) %>% 
  kable_styling()

```
:::

## How Often is the Strategy the Optimal?

-   We can now plot these data:
    -   **x-axis**: $\lambda$.
    -   **y-axis**: Fraction/percent of the time each strategy is optimal.
-   This is the Cost-Effectiveness Acceptability Curve

## Cost Effectiveness Acceptability Curve (CEAC)

```{r}
plot.ceac(ceac_obj, frontier=FALSE) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme(legend.position = c(0.82, 0.5))
```

# Interactive Amua Session 

## What Does the CEAC Tell Us?

-   Fenwick et al. (2001) the probability of being cost-effective cannot be used to determine the optimal option.

-   If the objective is to maximize health gain, decisions should be made based on *expected* net benefit, regardless of the uncertainty associated with the decision.

## Cost Effectiveness Acceptability Frontier

-   Layer you can add to the CEAC.
-   Shows the probability that the optimal option is cost-effective at different $\lambda$ values.
-   The CEAF is not necessarily the top "envelope" or region of the CEAC!

## Cost Effectiveness Acceptability Frontier

```{r}
plot.ceac(ceac_obj, frontier=TRUE) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme(legend.position = c(0.82, 0.5))
```

## Cost Effectiveness Acceptability Frontier

::: incremental
1.  Determine average costs and QALY for each strategy across *all* PSA iterations.
2.  Calculate NMB/NHB based on these averages.
3.  Determine optimal strategy based on the strategy that maximizes NMB/NHB.
:::

## Cost Effectiveness Acceptability Frontier

::: incremental
4.  Repeat for a range of values of $\lambda$.
5.  For each strategy find the range of values of $\lambda$ for which that strategy is optimal.

-   This determines the "switch points" of the CEAF.
:::

## Cost Effectiveness Acceptability Frontier

::: incremental
6.  The lowest value of $\lambda$ for which a given strategy is optimal is $\approx$ ICER for that strategy.

7.  The highest value of $\lambda$ for which a given strategy is optimal is the ICER for the next most costly option.
:::

## Cost Effectiveness Acceptability Frontier

```{r}
plot.ceac(ceac_obj, frontier=TRUE) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme(legend.position = c(0.82, 0.5))
```

## Expected Value of Perfect Information

Recall the two questions from the beginning of this talk:

1.  Which strategies are cost-effective given constraints and values and *based on current evidence*?
2.  Should we invest more resources to reduce uncertainty in our decisions?

## Expected Value of Perfect Information

-   CEAC and CEAF provide information on the degree to which uncertainty informs question 1.

-   These plots can help give us a sense of whether more research to reduce uncertainty may be valuable (Question 2).

-   **Value of Information** analyses provide a more concrete answer to Question 2.

## Expected Value of Perfect Information {.smaller}

::: columns
::: column
::: incremental
-   We will not cover VOI methods in detail here, but short courses are available.
-   Figure shows an instance where model is sensitive to uncertainty, but decisions are not.
-   It's not really worth pursuing additional research because we make the same decision regardless of the parameter values.
:::
:::

::: column
::: {style="padding-top: 150px;"}
![](images/paste-BB09C5B7.png)
:::
:::
:::

## Expected Value of Perfect Information {.smaller}

::: columns
::: column
::: incremental
-   If **decisions** are sensitive to uncertainty, then the value of information is high.
-   It may be worth pursuing additional research to reduce model parameter uncertainty.
:::
:::

::: column
::: {style="padding-top: 150px;"}
![](images/paste-0B14886B.png)
:::
:::
:::

## Expected Value of Perfect Information

-   You can use VOI methods with your PSA sample to rank-order parameters in terms of their importance in informing **decision uncertainty**.

-   Next slides briefly show you how to construct one VOI measure: the **expected value of perfect information**.

## Expected Value of Perfect Information

-   Idea: What is the value of reducing **all** uncertainty in the model?
-   Provides a rough sense of whether additional research should be pursued.
-   A related concept, the **expected value of partial perfect information,** can be constructed to tell us *which parameters* (or sets of parameters) we should focus on.

## Expected Value of Perfect Information

-   The "ingredients" for calculating the EVPI for a given $\lambda$ value are all in the CEAC and CEAF inputs.

## Expected Value of Perfect Information

::: incremental
1.  Determine the *overall* optimal strategy based on NMB as determined by average costs and QALYs across *all* PSA model runs.

-   Call this strategy $s^*$ (e.g., $s^*=D$)
-   NMB for this strategy is $\overline{NMB}(s^*)$.
:::

## Expected Value of Perfect Information

::: incremental
2.  In each PSA iteration, find the optimal strategy based on the NMB for each strategy *in that particular iteration*.

-   Call this strategy $s_m$ (e.g., $s_m=B$)
-   NMB for this strategy is $NMB_m(s_m)$.
:::

## Expected Value of Perfect Information

-   Now let's think about the economic consequences of $\overline{NMB}(s^*)$ and $NMB_m(s_m)$

## Expected Value of Perfect Information

-   On average, we would select strategy $s^*$ because it results in the highest expected health gain (i.e., it maximizes $\overline{NMB}(s^*)$).

-   But what if that decision is wrong?

## Expected Value of Perfect Information

-   The difference between $NMB_m(s_m)$ and $\overline{NMB}(s^*)$ for any PSA iteration provides an estimate of the **opportunity cost** of making the wrong decision.

-   If $s_m=s^*$, then $s_m - s^* = 0$.

    -   There is no opportunity cost of making the wrong decision!

## Expected Value of Perfect Information

-   The difference between $NMB_m(s_m)$ and $\overline{NMB}(s^*)$ for any PSA iteration provides an estimate of the **opportunity cost** of making the wrong decision.

-   If $s_m>s^*$, then $s_m - s^* > 0$.

    -   There is an opportunity cost to making the wrong decision.

## Expected Value of Perfect Information

-   The average value of $s_m - s^*$ in our PSA sample is the **expected value of perfect information (EVPI)**

-   It summarizes the degree to which there is an oportunity cost to making the wrong decision in our model.

## Expected Value of Perfect Information

-   Just as we did with the CEAC and CEAF, you can calculate an EVPI value for various $\lambda$ and construct a EVPI curve.

## Expected Value of Perfect Information

```{r}
plot.evpi(evpi)
```

## Expected Value of Perfect Information

::: columns
::: column
-   At $\lambda$=\$100,000/QALY, there is **high** value of information.
-   Our decision to implement one strategy over another is sensitive to uncertainty in our model.
:::

::: column
::: {style="padding-top: 150px;"}
```{r}
plot.evpi(evpi)
```
:::
:::
:::

## Expected Value of Perfect Information

::: columns
::: column
-   Note that our ICER for strategy C is very close to \$100,000.
-   At a decision threshold of $\lambda$ = \$100,000/QALY, different values for model parameters could result in adoption (i.e., ICER \< $\lambda$) or nonadoption of strategy C.
:::

::: column
::: {style="font-size: 0.8em"}
```{r}
rownames(df_cea) <- NULL
df_cea %>% 
  select(Strategy,Cost, Effect, ICER, Status) %>% 
  kable(digits = 3) %>% 
  kable_styling()
```
:::
:::
:::

## Expected Value of Perfect Information

::: columns
::: column
-   At $\lambda$=\$10,000/QALY, there is **low** value of information.
-   Our decision to implement one strategy over another is **not** sensitive to uncertainty in our model.
:::

::: column
::: {style="padding-top: 150px;"}
```{r}
plot.evpi(evpi)
```
:::
:::
:::

# Interactive Amua Session 

<!-- ## Sampling Parameters -->

<!-- ```{r, eval = FALSE} -->

<!-- #| echo: false -->

<!-- #| message: false -->

<!-- #| warning: false -->

<!-- library(randtoolbox) -->

<!-- library(gganimate) -->

<!-- library(ggthemes) -->

<!-- N = 1000 -->

<!-- int = 0.05 -->

<!-- get_mc <- function(N) { -->

<!--   data.frame(X1 = runif(N), -->

<!--              X2= runif(N)) %>%  -->

<!--   as_tibble() -->

<!-- } -->

<!-- get_hs <- function(N) { -->

<!--   halton(N,dim=2) %>%  -->

<!--   data.frame() %>%  -->

<!--   as_tibble() -->

<!-- } -->

<!-- get_coverage <- function(df_, int) { -->

<!--   df_ %>%  -->

<!--     mutate(X1_r = plyr::round_any(X1,int,ceiling), -->

<!--          X2_r = plyr::round_any(X2,int,ceiling)) %>%  -->

<!--   select(X1_r,X2_r) %>%  -->

<!--   mutate(n=1) %>%   -->

<!--   right_join( -->

<!--     crossing(X1_r = seq(0,1,int), X2_r = seq(0,1,int)) , c("X1_r","X2_r") -->

<!--   ) %>%  -->

<!--   mutate(n = ifelse(is.na(n),0,n)) %>%  -->

<!--   arrange(X1_r, X2_r) %>%  -->

<!--   ungroup() %>%  -->

<!--   summarise(n = mean(n)) %>%  -->

<!--   pull(n) -->

<!-- } -->

<!-- get_mc(N=1000) %>% get_coverage(int=0.03) -->

<!-- get_hs(N=1000) %>% get_coverage(int=0.03) -->

<!-- get_hs(N=10)  %>%  -->

<!--   mutate(iteration = row_number()) %>%  -->

<!--   mutate(end = 100) %>%  -->

<!--   ggplot(aes(x = X1, y = X2)) +  -->

<!--   geom_point() +  -->

<!--   theme_bw() +  -->

<!--   transition_events(start = iteration, end = end) -->

<!-- data <- data.frame( -->

<!--   x = 1:10, -->

<!--   y = runif(10), -->

<!--   begin = runif(10, 1, 100), -->

<!--   length = 1000, -->

<!--   enter = 100, -->

<!--   exit = 100 -->

<!-- ) -->

<!-- N = 75 -->

<!-- set.seed(123) -->

<!-- anim_mc <- get_mc(N) %>%  -->

<!--   mutate( -->

<!--     begin = runif(N, 1, N*10), -->

<!--     length = 200, -->

<!--     enter = runif(N,1,N), -->

<!--     exit = 1000 -->

<!--   ) %>%  -->

<!--   ggplot( aes(x=X1, y=X2)) + -->

<!--   geom_point(size = 2) + -->

<!--   labs(x = "Parameter 1", -->

<!--        y = "Parameter 2") + -->

<!--   theme_bw() + -->

<!--   transition_events(start = begin, -->

<!--                     end = begin + length, -->

<!--                     enter_length = enter, -->

<!--                     exit_length = exit) + -->

<!--  enter_grow()  -->

<!-- anim_save("lectures/media/lec_psa/animated-monte-carlo.gif", anim_mc) -->

<!-- set.seed(123)   -->

<!-- anim_hs <-  -->

<!--   get_hs(N) %>%  -->

<!--   mutate( -->

<!--     begin = runif(N, 1, N*10), -->

<!--     length = 200, -->

<!--     enter = runif(N,1,N), -->

<!--     exit = 1000 -->

<!--   ) %>%  -->

<!--   ggplot( aes(x=X1, y=X2)) + -->

<!--   geom_point(size = 2) + -->

<!--   labs(x = "Parameter 1", -->

<!--        y = "Parameter 2") + -->

<!--   theme_bw() + -->

<!--   transition_events(start = begin, -->

<!--                     end = begin + length, -->

<!--                     enter_length = enter, -->

<!--                     exit_length = exit) + -->

<!--  enter_grow()  -->

<!-- anim_save("lectures/media/lec_psa/animated-halton.gif", anim_hs) -->

<!-- ``` -->

<!-- ::: columns -->

<!-- ::: {.column width="50%"} -->

<!-- ![Monte Carlo Draws](media/lec_psa/animated-monte-carlo.gif){fig-align="center" layout-valign="center"} -->

<!-- ::: -->

<!-- ::: {.column width="50%"} -->

<!-- ![Halton Sequence](media/lec_psa/animated-halton.gif){fig-align="center" layout-valign="center"} -->

<!-- ::: -->

<!-- ::: -->

<!-- test -->
